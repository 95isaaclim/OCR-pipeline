{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2_page_1.pdf'), PosixPath('/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2_page_2.pdf'), PosixPath('/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2_page_3.pdf'), PosixPath('/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2_page_4.pdf')]\n",
      "Output written to D:\\Engineer\\Aws - sagemaker\\sagemaker-mob\\extracted_table_3.csv\n"
     ]
    }
   ],
   "source": [
    "#table extraction from hotel check-in documents using Amazon textract\n",
    "\n",
    "import boto3\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import os\n",
    "\n",
    "textractor = boto3.client(\"textract\", region_name=\"ap-southeast-1\")\n",
    "\n",
    "file_path = Path(r\"/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2.pdf\")\n",
    "\n",
    "if not file_path.exists():\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "def split_pdf(file_path):\n",
    "    reader = PdfReader(str(file_path))\n",
    "    output_paths = []\n",
    "    for i in range(len(reader.pages)):\n",
    "        writer = PdfWriter()\n",
    "        writer.add_page(reader.pages[i])\n",
    "        output_path = file_path.with_name(f\"{file_path.stem}_page_{i+1}.pdf\")\n",
    "        with open(output_path, \"wb\") as output_pdf:\n",
    "            writer.write(output_pdf)\n",
    "        output_paths.append(output_path)\n",
    "    return output_paths\n",
    "\n",
    "page_paths = split_pdf(file_path)\n",
    "\n",
    "print(page_paths)\n",
    "\n",
    "def extract_tables_from_page(page_path):\n",
    "    with open(page_path, \"rb\") as document:\n",
    "        imageBytes = bytearray(document.read())\n",
    "\n",
    "    response = textractor.analyze_document(\n",
    "        Document={\"Bytes\": imageBytes}, FeatureTypes=[\"TABLES\"]\n",
    "    )\n",
    "\n",
    "    blocks = response[\"Blocks\"]\n",
    "    tables = []\n",
    "\n",
    "    for block in blocks:\n",
    "        if block[\"BlockType\"] == \"TABLE\":\n",
    "            table = []\n",
    "            cell_map = {}\n",
    "            for relationship in block.get(\"Relationships\", []):\n",
    "                if relationship[\"Type\"] == \"CHILD\":\n",
    "                    for id in relationship[\"Ids\"]:\n",
    "                        cell_map[id] = None\n",
    "\n",
    "            for cell in blocks:\n",
    "                if cell[\"BlockType\"] == \"CELL\" and cell[\"Id\"] in cell_map:\n",
    "                    row_index = cell[\"RowIndex\"]\n",
    "                    col_index = cell[\"ColumnIndex\"]\n",
    "                    text = \"\"\n",
    "                    for relationship in cell.get(\"Relationships\", []):\n",
    "                        if relationship[\"Type\"] == \"CHILD\":\n",
    "                            for id in relationship[\"Ids\"]:\n",
    "                                for item in blocks:\n",
    "                                    if item[\"Id\"] == id and item[\"BlockType\"] == \"WORD\":\n",
    "                                        text += item[\"Text\"] + \" \"\n",
    "                    text = text.strip()\n",
    "                    cell_map[cell[\"Id\"]] = (row_index, col_index, text)\n",
    "                    table.append((row_index, col_index, text))\n",
    "\n",
    "            tables.append(table)\n",
    "    return tables\n",
    "\n",
    "csv_file_path = Path(r\"D:\\Engineer\\Aws - sagemaker\\sagemaker-mob\\extracted_table_3.csv\")\n",
    "\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for page_path in page_paths:\n",
    "        tables = extract_tables_from_page(page_path)\n",
    "        for table in tables:\n",
    "            max_row = max(cell[0] for cell in table)\n",
    "            max_col = max(cell[1] for cell in table)\n",
    "            table_data = [[\"\" for _ in range(max_col)] for _ in range(max_row)]\n",
    "\n",
    "            for cell in table:\n",
    "                row_index, col_index, text = cell\n",
    "                table_data[row_index - 1][col_index - 1] = text\n",
    "\n",
    "            writer.writerows(table_data)\n",
    "            writer.writerow([]) \n",
    "\n",
    "print(f\"Output written to {csv_file_path}\")\n",
    "\n",
    "for i in page_paths:\n",
    "    os.remove(Path(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    aws_access_key_id=os.getenv(\"aws_access_key_id\"),\n",
    "    aws_secret_access_key=os.getenv(\"aws_secret_access_key\"),\n",
    "    region_name=\"ap-southeast-2\",\n",
    ")\n",
    "\n",
    "#/Users/isaaclim/Downloads/bedrocktest.py\n",
    "import textwrap\n",
    "from string import Template\n",
    "\n",
    "insight_summarize = Template(\n",
    "    textwrap.dedent(\n",
    "        \"\"\"\n",
    "**Personality**\n",
    "You are a personal assistant to a hotel concierge, your job is to determine whether the given dataframes carry the descriptions of\n",
    "different relevant topics which include: \"ROOM CATEGORY\", \"MEAL\", \"TRANSPORT\", \"EVENT\". This can be determined by simply viewing the column\n",
    "titles in each dataframe given to you. If the dataframe columns don't include any of the  above, simply output the integer 0.\n",
    "\n",
    "**Table prompt**\n",
    "$insight\n",
    "\n",
    "**Input description**:\n",
    "You will receive a list of strings consisting of the column titles of a dataframe,  \\\n",
    "You might receive input in languages other than English.\n",
    "\n",
    "**Step description**:\n",
    "IMPORTANT: Explicitly output a single integer that best describes the dataframe's contents based on its column titles: \n",
    "\n",
    "1 if \"ROOM CATEGORY\" is the subject of the dataframe\n",
    "2 if \"MEAL\" is the subject of the dataframe\n",
    "3 if \"TRANSPORT\" is the subject of the dataframe\n",
    "4 if \"EVENT\" is the subject of the dataframe\n",
    "\n",
    "if none of these words can be found within the given dataframe column titles, output 0.\n",
    "\n",
    "**Output description**:\n",
    "The output should strictly only be the following integers: 0, 1, 2, 3, 4\n",
    "\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2_page_1.pdf'), PosixPath('/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2_page_2.pdf'), PosixPath('/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2_page_3.pdf'), PosixPath('/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2_page_4.pdf')]\n",
      "Output written to /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_1.csv\n",
      "Invocation details:\n",
      "- The input length is 332 tokens.\n",
      "- The output length is 5 tokens.\n",
      "- The model returned 1 response(s):\n",
      "1\n",
      "Output written to /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_2.csv\n",
      "Invocation details:\n",
      "- The input length is 343 tokens.\n",
      "- The output length is 5 tokens.\n",
      "- The model returned 1 response(s):\n",
      "0\n",
      "Output written to /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_3.csv\n",
      "Invocation details:\n",
      "- The input length is 324 tokens.\n",
      "- The output length is 5 tokens.\n",
      "- The model returned 1 response(s):\n",
      "3\n",
      "Output written to /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_4.csv\n",
      "Invocation details:\n",
      "- The input length is 343 tokens.\n",
      "- The output length is 5 tokens.\n",
      "- The model returned 1 response(s):\n",
      "2\n",
      "Output written to /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_5.csv\n",
      "Invocation details:\n",
      "- The input length is 339 tokens.\n",
      "- The output length is 5 tokens.\n",
      "- The model returned 1 response(s):\n",
      "4\n",
      "Output written to /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_6.csv\n",
      "Invocation details:\n",
      "- The input length is 332 tokens.\n",
      "- The output length is 5 tokens.\n",
      "- The model returned 1 response(s):\n",
      "0\n",
      "Output written to /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_7.csv\n",
      "Invocation details:\n",
      "- The input length is 328 tokens.\n",
      "- The output length is 5 tokens.\n",
      "- The model returned 1 response(s):\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#LLM integration to categorize each extracted table in preparation for final templating.\n",
    "\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "relevant = {}\n",
    "\n",
    "# Initialize AWS Textract client\n",
    "textractor = boto3.client(\"textract\", region_name=\"ap-southeast-1\")\n",
    "\n",
    "# Define the PDF file path\n",
    "file_path = Path(r\"/Users/isaaclim/Desktop/OCR/mentis-ocr/cth2.pdf\")\n",
    "#file_path = Path(r\"/Users/isaaclim/Desktop/OCR/mentis-ocr/Contract 3.pdf\")\n",
    "\n",
    "\n",
    "if not file_path.exists():\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "# Function to split PDF into individual pages\n",
    "def split_pdf(file_path):\n",
    "    reader = PdfReader(str(file_path))\n",
    "    output_paths = []\n",
    "    for i in range(len(reader.pages)):\n",
    "        writer = PdfWriter()\n",
    "        writer.add_page(reader.pages[i])\n",
    "        output_path = file_path.with_name(f\"{file_path.stem}_page_{i+1}.pdf\")\n",
    "        with open(output_path, \"wb\") as output_pdf:\n",
    "            writer.write(output_pdf)\n",
    "        output_paths.append(output_path)\n",
    "    return output_paths\n",
    "\n",
    "page_paths = split_pdf(file_path)\n",
    "\n",
    "print(page_paths)\n",
    "\n",
    "# Function to extract tables from each PDF page\n",
    "def extract_tables_from_page(page_path):\n",
    "    with open(page_path, \"rb\") as document:\n",
    "        imageBytes = bytearray(document.read())\n",
    "\n",
    "    response = textractor.analyze_document(\n",
    "        Document={\"Bytes\": imageBytes}, FeatureTypes=[\"TABLES\"]\n",
    "    )\n",
    "\n",
    "    blocks = response[\"Blocks\"]\n",
    "    tables = []\n",
    "\n",
    "    for block in blocks:\n",
    "        if block[\"BlockType\"] == \"TABLE\":\n",
    "            table = []\n",
    "            cell_map = {}\n",
    "            for relationship in block.get(\"Relationships\", []):\n",
    "                if relationship[\"Type\"] == \"CHILD\":\n",
    "                    for id in relationship[\"Ids\"]:\n",
    "                        cell_map[id] = None\n",
    "\n",
    "            for cell in blocks:\n",
    "                if cell[\"BlockType\"] == \"CELL\" and cell[\"Id\"] in cell_map:\n",
    "                    row_index = cell[\"RowIndex\"]\n",
    "                    col_index = cell[\"ColumnIndex\"]\n",
    "                    text = \"\"\n",
    "                    for relationship in cell.get(\"Relationships\", []):\n",
    "                        if relationship[\"Type\"] == \"CHILD\":\n",
    "                            for id in relationship[\"Ids\"]:\n",
    "                                for item in blocks:\n",
    "                                    if item[\"Id\"] == id and item[\"BlockType\"] == \"WORD\":\n",
    "                                        text += item[\"Text\"] + \" \"\n",
    "                    text = text.strip()\n",
    "                    cell_map[cell[\"Id\"]] = (row_index, col_index, text)\n",
    "                    table.append((row_index, col_index, text))\n",
    "\n",
    "            tables.append(table)\n",
    "    return tables\n",
    "\n",
    "# Counter for CSV file names\n",
    "table_counter = 1\n",
    "\n",
    "# Extract tables and write each table to a different CSV file\n",
    "for page_path in page_paths:\n",
    "    tables = extract_tables_from_page(page_path)\n",
    "    for table in tables:\n",
    "        max_row = max(cell[0] for cell in table)\n",
    "        max_col = max(cell[1] for cell in table)\n",
    "        table_data = [[\"\" for _ in range(max_col)] for _ in range(max_row)]\n",
    "\n",
    "        for cell in table:\n",
    "            row_index, col_index, text = cell\n",
    "            table_data[row_index - 1][col_index - 1] = text\n",
    "\n",
    "        # Define the CSV file path dynamically\n",
    "        csv_file_path = file_path.with_name(f\"extracted_table_{table_counter}.csv\")\n",
    "        \n",
    "        # Write the table data to the CSV file\n",
    "        with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerows(table_data)\n",
    "\n",
    "        print(f\"Output written to {csv_file_path}\")\n",
    "        \n",
    "        df_curr = pd.read_csv(csv_file_path)\n",
    "        column_list = df_curr.columns\n",
    "            \n",
    "        # Increment the table counter\n",
    "        table_counter += 1\n",
    "\n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 2048,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": insight_summarize.substitute(insight=column_list),\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"What's the category of the contents within this dataframe based on its given column titles?\",\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        \n",
    "        try:\n",
    "            response = client.invoke_model(\n",
    "                modelId=model_id,\n",
    "                body=json.dumps(request_body),\n",
    "            )\n",
    "        \n",
    "            # Process and print the response\n",
    "            result = json.loads(response.get(\"body\").read())\n",
    "            input_tokens = result[\"usage\"][\"input_tokens\"]\n",
    "            output_tokens = result[\"usage\"][\"output_tokens\"]\n",
    "            output_list = result.get(\"content\", [])\n",
    "        \n",
    "            print(\"Invocation details:\")\n",
    "            print(f\"- The input length is {input_tokens} tokens.\")\n",
    "            print(f\"- The output length is {output_tokens} tokens.\")\n",
    "        \n",
    "            print(f\"- The model returned {len(output_list)} response(s):\")\n",
    "            for output in output_list:\n",
    "                print(output[\"text\"])\n",
    "        except ClientError as err:\n",
    "            logger.error(\n",
    "                \"Couldn't invoke Claude 3 Sonnet. Here's why: %s: %s\",\n",
    "                err.response[\"Error\"][\"Code\"],\n",
    "                err.response[\"Error\"][\"Message\"],\n",
    "            )\n",
    "\n",
    "        csv_file_path = str(csv_file_path)\n",
    "        \n",
    "        if (output[\"text\"] != \"0\") & (len(output[\"text\"]) == 1):\n",
    "            relevant[csv_file_path] = output[\"text\"]\n",
    "            \n",
    "      \n",
    "# Clean up temporary split PDF files\n",
    "for i in page_paths:\n",
    "    os.remove(Path(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant =  dict(sorted(relevant.items(), key=lambda item: item[1]))\n",
    "for key, value in relevant.items():\n",
    "    if value == \"1\":\n",
    "        relevant[key] = \"ROOM CATEGORY\"\n",
    "    elif value == \"2\":\n",
    "         relevant[key] = \"MEAL\"\n",
    "    elif value == \"3\":\n",
    "         relevant[key] = \"TRANSPORT\"\n",
    "    elif value == \"4\":\n",
    "         relevant[key] = \"EVENT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_1.csv, ROOM CATEGORY\n",
      " /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_4.csv, MEAL\n",
      " /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_3.csv, TRANSPORT\n",
      " /Users/isaaclim/Desktop/OCR/mentis-ocr/extracted_table_5.csv, EVENT\n"
     ]
    }
   ],
   "source": [
    "for key, value in relevant.items():\n",
    "     print(f\" {key}, {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final table arrangement\n",
    "\n",
    "file_path = '/Users/isaaclim/Desktop/OCR/mentis-ocr/final_template_trial.csv'\n",
    "\n",
    "for key, value in relevant.items():\n",
    "    df = pd.read_csv(key)\n",
    "\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write('\\n')\n",
    "        f.write(value + '\\n')\n",
    "         \n",
    "    df.to_csv(file_path, mode='a', header=True, index=False)\n",
    "    \n",
    "    # Step 4: Write an empty row after the DataFrame\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write('\\n' + \" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
